\section{Conclusions}\label{conclusions}
A generalized mathematical framework for proposing a constitutive relationship and fitting the physical constants associated with that constitutive relationship to a data corpus using the generalized fitting framework provided by machine learning/artificial intelligence has been presented. The proposed method maps between an arbitrary constitutive relationship and an artificial neural network model. The resulting covector spaces (coefficients) of the series are colinear. The generating function for the constitutive relationship is in terms of physical constants while that of neural network is determined through the trained model parameters. The method of least-squares is used to fit the physical constants to the trained neural network model parameters through this colinear covector space.

A simplified yield strength model, which includes flow stress, solute concentration, and Hall-Petch strengthening, is provided as an example to demonstrate the more general form for constructing the vector space for these two models.

Neural network series expansions are constructed layer-wise, which allows this framework to handle arbitrarily complex network architectures, including drop out, whitening, and any element-wise activation function. Mathematical descriptions of the rectified linear unit (ReLU) activation commonly used in neural network hidden layers, linear activation for regression networks, and softmax activation for classification networks are derived.

A mathematical framework to create constitutive relationship series expansions is also provided. Select coefficient generating functions for functional forms commonly found in materials physics are presented in the Appendix (Table~\ref{tab:generating functions of common functions}) to serve as a starting point, but any other functions that can be described by a polynomial series may be used.

As a single framework capable of describing arbitrarily complex relationships, this approach is intended to facilitate fits between existing data and any hypothesized constitutive relationships built upon the same vector space as the trained neural network model.
