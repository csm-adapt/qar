\section{Appendix}\label{sec:appendix}


% Begin vector layer expansion moved by Andrew and Maria on May 1st. %
Suppose that $\mathbf{x} \in \mathbb{R}^d, (d \in \mathbb{Z})$ is a vector, $\mathbf{y} \in \mathbb{R}^c, (c \in \mathbb{Z})$ is a vector, $\mathbf{\Theta} \in \mathbb{R}^{c \times d}$ is a matrix, and $\sigma: \mathbb{R} \to \mathbb{R}$ is an analytic function. Since $\sigma$ is analytic, it can be represented as

\begin{align}
    \sigma(x)
    &= s_0 + s_1 x + s_2 x^2 + \cdots \nonumber \\
    &= \sum_{k=0}^{\infty} s_{k} x^{k}.
    \label{eqn:vector layer activation}
\end{align}

Suppose that we can represent each entry of $\mathbf{x}$ as a power series of the entries of a vector $\mathbf{z} \in \mathbb{R}^{w}, (w \in \mathbb{Z})$.

\begin{align}
    \forall i &= 1, \cdots, d, \nonumber \\ x_i
    &= a^{(i)}_{0,0,\cdots,0} + a^{(i)}_{1,0,\cdots,0} z_1 + a^{(i)}_{2,0,\cdots,0} z_1^2 + \cdots \nonumber \\
    &+ a^{(i)}_{0,1,\cdots,0} z_2 + a^{(i)}_{1,1,\cdots,0} z_1 z_2 + a^{(i)}_{2,1,\cdots,0} z_1^2 z_2 + \cdots \nonumber \\
    &+ a^{(i)}_{0,0,\cdots,1} z_w + a^{(i)}_{1,0,\cdots,1} z_1 z_w + a^{(i)}_{2,0,\cdots,1} z_1^2 z_w + \cdots \nonumber \\
    &= \sum_{n_1,n_2,\cdots,n_w}^{\infty,\infty,\cdots,\infty} a^{(i)}_{n_1,n_2,\cdots,n_w} z_1^{n_1} z_2^{n_2} \cdots z_w^{n_w}.
    \label{eqn:vector x series}
\end{align}

Finally, suppose that we have the relation

\begin{equation}
    \mathbf{y} = \mathbf{\sigma}(\mathbf{\Theta} \mathbf{x})
    \label{eqn:vector layer relation}
\end{equation}

where $\sigma$ is being applied element-wise to a vector.\\

Similar to the scalar case, this represents a multi-neuron layer of an ANN with input $\mathbf{x}$, weights $\mathbf{\theta}$, output $\mathbf{y}$, and activation $\sigma$. The value of $z$ can be interpreted as the input to the entire ANN.

\subsubsection{Objective}

We wish to represent each entry of $\mathbf{y}$ as a power series of the entries of $\mathbf{z}$. That is,

\begin{align}
    \forall i &= 1, \cdots, c, \nonumber \\ y_i
    &= b^{(i)}_{0,0,\cdots,0} + b^{(i)}_{1,0,\cdots,0} z_1 + b^{(i)}_{2,0,\cdots,0} z_1^2 + \cdots \nonumber \\
    &+ b^{(i)}_{0,1,\cdots,0} z_2 + b^{(i)}_{1,1,\cdots,0} z_1 z_2 + b^{(i)}_{2,1,\cdots,0} z_1^2 z_2 + \cdots \nonumber \\
    &+ b^{(i)}_{0,0,\cdots,1} z_w + b^{(i)}_{1,0,\cdots,1} z_1 z_w + b^{(i)}_{2,0,\cdots,1} z_1^2 z_w + \cdots \nonumber \\
    &= \sum_{n_1,n_2,\cdots,n_w}^{\infty,\infty,\cdots,\infty} b^{(i)}_{n_1,n_2,\cdots,n_w} z_1^{n_1} z_2^{n_2} \cdots z_w^{n_w}.
    \label{eqn:vector y series}
\end{align}

Rewriting Equation (\ref{eqn:vector layer relation}) in terms of Equations (\ref{eqn:vector layer activation}) and (\ref{eqn:vector x series}), we obtain

\begin{align}
    \forall i &= 1, \cdots, c, \nonumber \\ y_i
    &= \left[\mathbf{\sigma}(\mathbf{\Theta} \mathbf{x})\right]_i \nonumber \\
    &= \sigma(\mathbf{\theta}_i \mathbf{x}) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k (\mathbf{\theta}_i \mathbf{x})^k \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{j=1}^{d} \theta_{ij} x_{j}\right)^k \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} (\theta_{ij} x_j)^{k_j} \right) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d}\theta_{ij}^{k_j} \left(\sum_{n_1, \cdots, n_w}^{\infty,
    \cdots, \infty} a^{(j)}_{n_1,\cdots,n_w} z_1^{n_1}  \cdots z_w^{n_w} \right)^{k_j}\right) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} \theta_{ij}^{k_j} \left(\sum_{\lVert \mathbf{L} \rVert_1 = k_j} \binom{k_j}{\mathbf{L}} \prod_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} (a^{(j)}_{n_1, \cdots, n_w} z_1^{n_1} \cdots z_w^{n_w})^{l_{n_1, \cdots, n_w}} \right)\right). \nonumber \\
    \label{eqn:vector y expansion}
\end{align}

where $\mathbf{L}$ is a collection of non-negative integers that are indices such that

\begin{align*}
    &\mathbf{L} = l_{\underbrace{0, \cdots, 0}_{\times w}}, \ldots, l_{\underbrace{\infty, \cdots, \infty}_{\times w}}, \\
    &\lVert \mathbf{L} \rVert_1 = l_{0, \cdots, 0} + \cdots + l_{\infty, \cdots, \infty}, \\
    &\binom{k_j}{\mathbf{L}} = \binom{k_j}{l_{0, \cdots, 0}, \ldots, l_{\infty, \cdots, \infty}}.
\end{align*}

\subsubsection{Coefficient Extraction}

To find the coefficients $b^{(i)}_{m_1, \cdots, m_w}$ from Equation (\ref{eqn:vector y expansion}), we must find terms satisfying index constraints

\begin{align}
    k &= \sum_{n=1}^{d} k_n \\
    k_j &= \lVert \mathbf{L} \rVert_1
\end{align}

and power constraints

\begin{align}
    m_1 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_1 l_{n_1, \cdots, n_w} \nonumber \\
    m_2 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_2 l_{n_1, \cdots, n_w} \nonumber \\
    \vdots \;\; &= \qquad \vdots \nonumber \\
    m_3 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_w l_{n_1, \cdots, n_w}.
\end{align}

Similar to the scalar case, notice that the power constraints can be simplified since any solution must also satisfy

\begin{equation}
    l_{n_1, \cdots, n_{p-1}, m_p + 1, n_{p+1}, \cdots, n_w} = l_{n_1, \cdots, n_{p-1}, m_p + 2, n_{p+1}, \cdots, n_w} = \cdots = 0.
\end{equation}

resulting in

\begin{align}
    m_1 &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_1 l_{n_1, \cdots, n_w} \nonumber \\
    m_2 &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_2 l_{n_1, \cdots, n_w} \nonumber \\
    \vdots \;\; &= \qquad \vdots \nonumber \\
    m_w &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_w l_{n_1, \cdots, n_w}.
\end{align}

If a collection of $k_1, \ldots, k_d$ and $\mathbf{L}$ satisfy all of these constraints, then the value

\begin{equation*}
    s_k \binom{k}{k_1, \cdots, k_d} \binom{k_j}{\mathbf{L}} \theta_{ij}^{k_j} (a^{(j)}_{n_1, \cdots, n_w})^{l_{n_1, \cdots, n_w}}
\end{equation*}

may be pulled out to the sum of $b^{(i)}_{m_1, \cdots, m_w}$. In concise terms we have derived

{\color{red} At this point, it is infeasible to attempt to write Equation (\ref{eqn:scalar y expansion}) directly as a power series. Instead, we will extract the coefficients $b_i$ for $i = 0, 1, 2, \ldots$ by observing constraints on the coefficients.\\

First, as a result of the multinomial theorem, we have a constraint on the inner sum in Equation (\ref{eqn:scalar y expansion}), which is constraint on the index of the sum so we shall call this an index constraint.\\

Second, the coefficient $b_i$ is associated with the term $z^i$ so we must form an equality between $i$ and the power on $z$ in Equation (\ref{eqn:scalar y expansion}). To satisfy this equality, a constant is required. This is a constraint on the power of the scalar $z$ so we shall call this a power constraint. This constraint implies $k_{i + 1} = k_{i + 2} = \cdots = 0$. Therefore, both the index constraint and power constraint can be reduced to a finite series instead of an infinite series.}

\begin{equation}
    b^{(i)}_{m_1, \cdots, m_w} = \sum_{k=0}^{\infty} s_k \sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} \theta_{ij}^{k_j} \sum_{\substack{l_{0, \cdots, 0} + \cdots + l_{\infty, \cdots, \infty} = k_j \\ \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_1 l_{n_1, \cdots, n_w} = m_1 \\ \vdots \\ \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_w l_{n_1, \cdots, n_w} = m_w}} \binom{k_j}{\mathbf{L}} (a^{(j)}_{n_1, \cdots, n_w})^{l_{n_1, \cdots, n_w}}
\end{equation}

The only issue is that of finding solutions to the index and power constraints.
% End vector layer expansion moved by Andrew and Maria on May 1st. %

%Beginning of content moved to appendix from main body by Andrew and Maria on April 29th.%

%Beginning of softmax derivation %
\subsection{Softmax Activation}

For example, consider the Softmax activation function which is given in Equation (???)

\begin{equation}
	\sigma(\bold z)_i =
	\frac{e^{z_i}}{\sum\limits_{j=1}^k e^{z_j}}
\end{equation}

The goal is to obtain a activation generating function for the chosen activation function in the neural network.

The partion function, $\bold Z$, is defined below as
\begin{equation}
	\bold Z = \sum\limits_{j=1}^k e^{z_j}
\end{equation}
The series representation for the exponential is given by the following
\begin{equation}
	e^{z_{i}} = \sum_{k=0}^{\infty}\frac{1}{k!} z_{i}^{k}
\end{equation}
Therefore
\begin{equation}
	\sigma(\bold z)_i =
	\sum\limits_{k=0}^{\infty} \alpha_{k}z_{i}^{k}
\end{equation}
Where $\alpha_k = \frac{1}{{\bold Z}k!}$ is the generating function for the Softmax
%End of softmax derivation %

%Beginning of ReLU %
\subsection{ReLU Activation}

The ReLU (rectified linear units) function is another commonly used activation function and is given below in Equation~(\ref{eqn:ReLU}):
\begin{equation}
	f(x) =
	\begin{cases}
		x, & \text{if}\ x > 0 \\
		0, & \text{otherwise}
	\end{cases}
	\label{eqn:ReLU}
\end{equation}

The generating function for the ReLU is given in Equation~(\ref{eqn: ReLU generating function}) and it is dependent on both the input variables and the network weights.
\begin{equation}
	\alpha_n =
		\begin{cases}
			1 & \bold z > 0 \\
			0 & \text{otherwise}
		\end{cases}
	\label{eqn: ReLU generating function}
\end{equation}

Similarly, the linear generating function is given in Equation~(\ref{eqn: linear generating function}) but it is only dependent upon $\bold n$.
\begin{equation}
	\alpha_n =
		\begin{cases}
			1 & \bold n = 1 \\
			0 & \text{otherwise}
		\end{cases}
	\label{eqn: linear generating function}
\end{equation}

Although ReLU (rectified linear units) have become a more common activation function, its discontinuity at $x = 0$ requires an infinite series to fully capture the behavior at this transition.
%End of ReLU %

%Beginning of softplus %
%The Softplus activation function is similar to the ReLU activation function, but unlike the ReLU, the first derivative of the Softplus is continuous. As will be discussed later, the Softplus can be modified such that it can be forced to converge to the ReLU.
\begin{align*}
	f(x) & = log(1+e^x)\\
	& = \frac{1}{\alpha}log(1+e^{\alpha x}), \quad \alpha > 0 \\
	& = \frac{1}{\alpha}log(z), \quad z = 1 + e^{\alpha x} \\
	& = \frac{1}{\alpha}\sum\limits_{k=1}^\infty
	\frac{1}{k}\left(\frac{z-1}{z}\right)^k \\
	& = \frac{1}{\alpha}\sum\limits_{k=1}^\infty
	\frac{1}{k}(z-1)^{k} z^{-k} \\
	& = \frac{1}{\alpha}\sum\limits_{k=1}^\infty
	\frac{1}{k} e^{\alpha x k} (1 + e^{\alpha x})^{k} \\
	& = \frac{1}{\alpha}\sum\limits_{k=1}^\infty
	\frac{1}{k} \left(\sum\limits_{m=0}^\infty
	\frac{\alpha^{m} x^{m}}{m!}\right)^k \left(1 + \sum\limits_{n=0}^{k} \frac{\alpha^{n} x^{n}}{n!}\right)^{-k}
\end{align*}

From Gradshteyn and Ryzhik - 0.314 Power series raised to powers.
\begin{equation}
	\left(\sum\limits_{k=0}^{\infty}a_{k}x^{k}\right)^{n} = \sum\limits_{k=0}^{\infty}c_{k}x^{k} \\
	\label{eqn:power series raised to powers}
\end{equation}
where
\begin{align*}
	c_{0} = a_{0}^{n}, \quad c_{m} = \frac{1}{ma_{0}}\sum\limits_{k=0}^{\infty}c_{k}x^{k}
\end{align*}

And therefore it can be seen that

\begin{align*}
	c_{0} = \frac{\alpha^{0}}{0!}=1, \quad c_{l} = \frac{1}{l}\sum\limits_{j=1}^{l}(jk-l+j)\frac{\alpha^{l}}{l!}c_{l-j}
\end{align*}

\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\sum\limits_{m=0}^{\infty}c_{m}x^{m}\left( 1+\sum\limits_{n=0}^{\infty}\frac{\alpha^{n}x^{n}}{n!}\right)^{-k} \\
\end{align*}

From Gradshteyn and Ryzhik - 1.1 Power of Binomials, 1.111 Power Series
\begin{equation}
	(a+x)^n = \sum\limits_{k=0}^{n}{n \choose k}x^k a^{n-k}
\end{equation}

And similarly

\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\sum\limits_{m=0}^{\infty}c_mx^m
	\left[ \sum\limits_{i=0}^{k}{k \choose i} \left( \sum\limits_{n=0}^{\infty}\frac{\alpha^nx^n}{n!} \right)^{i} \right]^{-1}
\end{align*}

Once again the substitution for a power series raised to a power is applied.

\begin{align*}
	d_{0} = 1, \quad d_{l}=\frac{1}{l}\sum\limits_{j=1}^{l}(ji-l+j)\frac{\alpha^{l}}{l!}d_{l-j}
\end{align*}

\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\left[ \frac{\sum\limits_{m=0}^{\infty}c_{m}x^{m}}{\sum\limits_{i=0}^{k}\frac{1}{{k \choose i}}\sum\limits_{n=0}^{\infty}d_{n}x^{n}} \right]
\end{align*}

After some rearranging of term, the result is given below.

\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\left[ \frac{\sum\limits_{m=0}^{\infty}c_{m}x^{m}}{\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{k}\frac{1}{{k \choose i}}d_{n}x^{n}} \right] \\
\end{align*}

And therefore the following is obtained.
\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\left[ \frac{\sum\limits_{m=0}^{\infty}c_{m}x^{m}}{\sum\limits_{n=0}^{\infty}d_{n}'x^{n}} \right]
\end{align*}

Where
\begin{align*}
 	d_{n}' & = \sum\limits_{i=0}^{k}\frac{1}{{k \choose i}}d_{n}
\end{align*}

From Gradshteyn and Ryzhik - 0.313 Division of power series.

\begin{equation}
	\frac{\sum\limits_{k=0}^{\infty}b_k x^k}{\sum\limits_{k=0}^{\infty}a_k x^k}
	= \frac{1}{a_0}\sum\limits_{k=0}^{\infty}c_kx^k
\end{equation}

where

\begin{equation}
	c_n + \frac{1}{a_0}\sum\limits_{k=1}^n c_{n-k}a_k - b_n = 0
\end{equation}

And therefore

\begin{align*}
	b_{m} = \sum\limits_{j=1}^{m}c_{m}-b_{m-j}d_{j}
\end{align*}

\begin{align*}
	& = \frac{1}{\alpha}\sum\limits_{k=1}^{\infty}\frac{1}{k}\sum\limits_{n=0}^{\infty}b_{n}x^{n} \\
	& = \sum\limits_{n=0}^{\infty}\sum\limits_{k=1}^{\infty}\frac{b_{n}}{\alpha k}x^{n}=\frac{1}{\alpha}log(1+e^{\alpha x})
\end{align*}

where $\sum\limits_{k=1}^{\infty}\frac{b_{n}}{\alpha k}$ is the generating function for the softplus
%End of softplus %

%Beginning of scalar coefficient extraction %
At this point, it is infeasible to attempt to write Equation (\ref{eqn:scalar y expansion}) directly as a power series of $z$. Instead, we will extract the coefficients $b_i$ for $i = 0, 1, 2, \ldots$ by observing constraints on the coefficients.\\

First, as a result of the multinomial theorem, we have a constraint on the inner sum in Equation (\ref{eqn:scalar y expansion}) that is

\begin{equation}
	k = \sum_{n=0}^{\infty} k_n
	\label{eqn:scalar index constraint}
\end{equation}

where each $k_n$ must be a non-negative integer. This is a constraint on the index of the sum so we shall call this an index constraint.\\

Second, the coefficient $b_i$ is associated with the term $z^i$ so we must form an equality between $i$ and the power on $z$ in Equation (\ref{eqn:scalar y expansion}). To satisfy this equality, we note that

\begin{equation}
	C z^{0 k_0 + 1 k_1 + 2 k_2 + \cdots} = (a_0 z^0)^{k_0} (a_1 z^1)^{k_1} (a_2 z^2)^{k_2} \cdots = \prod_{n=0}^{\infty} (a_n z^n)^{k_n}
\end{equation}

where $C$ is some constant. This implies that

\begin{equation}
    i = \sum_{n=0}^{\infty} n k_n.
\end{equation}

This is a constraint on the power of the scalar $z$ so we shall call this a power constraint.\\

Notice that the power constraint implies $k_{i + 1} = k_{i + 2} = \cdots = 0$. Therefore, both the index constraint and power constraint can be reduced to a finite series instead of an infinite series.

\begin{equation}
	k = \sum_{n=0}^{i} k_n, \qquad i = \sum_{n=0}^{i} n k_n
\end{equation}

If a collection of $k_j$ for $j = 0, 1, 2, \ldots$ satisfy both of these constraints, then, the constant coefficients of $z^i$ which are

\begin{equation*}
    s_k \theta^k \binom{k}{k_0, k_1, \cdots, k_i} \prod_{n=0}^{\infty} a_n^{k_n}
\end{equation*}

may be pulled out and added to the sum of $b_i$. In concise terms we have derived that

\begin{equation}
    b_i = \sum_{k=0}^{\infty} s_k \theta^k \sum_{\substack{k_0 + k_1 + \cdots + k_i = k \\ 0 k_0 + 1 k_1 + \cdots + i k_i = i}} \binom{k}{k_0, k_1, \cdots, k_i} \prod_{n=0}^{\infty} a_n^{k_n}.
	\label{eqn:scalar power series result}
\end{equation}

Thus, we have derived a tractable method of computing the coefficients of a power series after a linear and nonlinear transformation. We have included some worked examples of these coefficients in Appendix \ref{sec:worked scalar coeffs}.
%End of scalar coefficient extraction %

%Beginning of Discussion of activation functions %
% ReLU activation depends on the data, others do not.
ReLU activation,
\[
    \sigma(z) = \begin{cases}
            z   & \mbox{if } z > 0, \\
            0   & \mbox{otherwise}
        \end{cases}
\]
is discontinuous in the first derivative at $z_i = 0$. Therefore, the coefficient generating function of this activation must either be either a function of the input data, ${\bf z}$ or a small modification must be made to the softplus,
\begin{equation}
    \sigma(z; \alpha) = \frac{1}{\alpha} \ln\left( 1 + e^{\alpha x} \right).
    \label{eqn:modified softmax}
\end{equation}
In the limit as $\alpha$ approaches infinity, this converges to the ReLU. Practically, though, $\alpha$ can be assigned a large value and the coefficient generating function no longer depends on the input data, see Equation~\ref{eqn:modified softplus generating function} in {\color{red} the appendix}. However, because of the high computational cost of expressing the coefficients using the modified softplus, and the relative low cost of forward evaluation of the trained neural network in order to apply Equation~\ref{eqn:ReLU generating function}, expressing the coefficient generating function of ReLU in terms of the input data, as in Equation~\ref{eqn:ReLU generating function}, rather than the training-data-agnostic approach in Equation~\ref{eqn:modified softplus generating function} would seem more practical.
%End of Discussion of activation functions %

%Beginning of coefficient example %
If ${\bf x} = ([c], d)$ are the concentration of a solute and grain size, respectively, then the constitutive relationship expansion would be,
\begin{align}
    \sigma_y &= \sigma_f  + C [c]^{2/3} + \frac{k}{\sqrt{d}} \nonumber \\
        &= \sigma_f + \underbrace{\sum_{k=0}^{\infty} a_k [c]^k}_{\text{solute}} + \underbrace{\sum_{k=0}^{\infty} b_k d^k}_{\text{Hall-Petch}} \nonumber \\
        &= \sigma_f + \sum_{k=0}^{\infty} a_k \he[\left( {\bf S}_\text{solute}{\bf x} \right)]{k} + \sum_{k=0}^{\infty} b_k \he[\left( {\bf S}_\text{Hall-Petch} {\bf x} \right)]{k}
    \label{eqn:simplified strengthening}
\end{align}
where
\begin{conditions}
    \sigma_f & Matrix flow stress. \\
    a_k & Coefficient generating function for $x^{2/3}$. \\
    b_k & Coefficient generating function for $x^{-1/2}$. \\
    {\bf S}_\text{solute} & Selection vector/matrix for choosing the solute concentration from the input vector. \\
    {\bf S}_\text{Hall-Petch} & Selection vector/matrix for choosing the grain size from the input vector.
\end{conditions}
(Coefficient generating functions can be found in Table~\ref{tab:generating functions of common functions}.) Combining Equations~\ref{eqn:simplified strengthening} and \ref{eqn:hadamard exponent vector},
\begin{multline*}
    \sigma_y = \sigma_f + \sum_{k=0}^\infty a_k \left[ \sum_{l_1=0}^k \binom{k}{l_1, l_2} \prod_{m=1}^2 (({\bf S}_\text{solute})_m {\bf x}_m)^{l_m} \right] \\
        + \sum_{k=0}^\infty b_k \left[ \sum_{l_1=0}^k \binom{k}{l_1, l_2} \prod_{m=1}^2 (({\bf S}_\text{Hall-Petch})_m {\bf x}_m)^{l_m} \right],\ l_2 = k - l_1
\end{multline*}
which further simplifies to
\begin{multline*}
    \sigma_y = \sigma_f + \sum_{k=0}^\infty \sum_{l_1=0}^k a_k \binom{k}{l_1, l_2} \prod_{m=1}^2 ({\bf S}_\text{solute})_m^{l_m} \prod_{m=1}^2{\bf x}_m^{l_m} \\
        + \sum_{k=0}^\infty \sum_{l_1=0}^k b_k \binom{k}{l_1, l_2} \prod_{m=1}^2 ({\bf S}_\text{Hall-Petch})_m^{l_m} \prod_{m=1}^2{\bf x}_m^{l_m}
\end{multline*}
such that the term $\prod_{m=1}^2{\bf x}_m^{l_m}$ serves as the common basis set over which the summation occurs, so that now, having a common basis, this simplifies to,
\begin{equation}
    \sigma_y = \sigma_f + \sum_{k=0}^\infty \sum_{l_1=0}^k \binom{k}{l_1, l_2} \left( a_k \prod_{m=1}^2 ({\bf S}_\text{solute})_m^{l_m} + b_k \prod_{m=1}^2 ({\bf S}_\text{Hall-Petch})_m^{l_m} \right) \prod_{m=1}^2{\bf x}_m^{l_m}
\end{equation}
% End of coefficent example %

%Begin element-wise exponential %
Equation~\ref{eqn:ANN power series coefficient generating function} shows that the polynomial series expansion for the first layer of a neural network,
\[
    y^{(1)} = \sum_k \alpha^{(1)}_k \he[\left( \boldsymbol{\theta}^{(1)} {\bf x} \right)]{k},
\]
similarly relies on the element-wise exponential, $(\bullet)\he{n}$, as does the expansion of all layers. Unlike scalar exponentiation, element-wise exponentiation does not distribute, as seen in Equation~\ref{eqn:nondistributive hadamard}, and because element-wise exponentiation does not distribute, this equation explicitly captures all possible (second, $x_i^m x_j^n$; third, $x_i^m x_j^n x_k^p$; fourth, $x_i^m x_j^n x_k^p x_l^r$; etc.) cross-interactions of each term in $(\boldsymbol{\theta}{\bf x})$ at all polynomial orders. This is equivalent, then, to expanding over the basis set that includes all cross-interactions in the input vector space for both the constitutive relation and neural network polynomial expansions.
%End element-wise exponential %

%Begin data pre-processing %
Data preprocessing is an important step in training a neural network to avoid implicit bias. Commonly, data is whitened, also known as scaling or standardization, ${\bf z}_s^{(k)}: {\bf z}_s^{(k)} = \frac{{\bf z}^{(k)} - \overline{{\bf z}^{(k)}}}{\sigma}$, where $\overline{{\bf z}^{(k)}}$ is the arithmetic mean and $\sigma$ the standard deviation of data in layer, $k$. However, a model trained on such scaled data would no longer share the vector space of the constitutive relationship. To ensure that both the neural network and constitutive relation expansions share a common vector space, and thus a common covector space, this whitening procedure must be integrated into the construction of the neural network architecture.

Procedurally, the neural network expansion proceeds as described in Section~\ref{methods}. The input to each layer, which is the output from the previous layer, is subjected first to an affine transformation, then to an activation function. The activation function is completely described though its polynomial expansion and the corresponding coefficient generating function. Using this same structure, then, data whitening can be applied to any layer, including the input layer, as an identity tranform ($\boldsymbol{\theta} = {\bf I}_d$, where $d$ is the dimension of the source layer) followed by the whitening expansion whose coefficient generating function is simply,
\begin{equation}
    \alpha_k({\bf z}) = \begin{cases}
        -\overline{\bf z}/\sigma    & \mbox{if } k = 0, \\
        1/\sigma                    & \mbox{if } k = 1, \\
        0                           & \mbox{otherwise}
    \end{cases}
\end{equation}
where
\begin{conditions}
    \overline{\bf z}    & The mean of the data into the standarization layer. \\
    \sigma              & The standard deviation of the data into the standardization layer.
\end{conditions}
By introducing such a whitening layer, data standardization can be included at any point in the neural network architecture.
%End data pre-processing %


%Beginning of vector coefficients derivation.%
\subsection{Polynomial Expansion of a Vector Layer}
\label{sec:polynomial series vector}

This derivation will be a generalization of the derivation given in Section \ref{sec:polynomial series scalar} where each layer of the ANN may have an arbitrary number of neurons. We show a similar result to before where we find coefficients for a power series after linear and nonlinear transformations.

\subsubsection{Notation}

Suppose that $\mathbf{x} \in \mathbb{R}^d, (d \in \mathbb{Z})$ is a vector, $\mathbf{y} \in \mathbb{R}^c, (c \in \mathbb{Z})$ is a vector, $\mathbf{\Theta} \in \mathbb{R}^{c \times d}$ is a matrix, and $\sigma: \mathbb{R} \to \mathbb{R}$ is an analytic function. Since $\sigma$ is analytic, it can be represented as

\begin{align}
    \sigma(x)
    &= s_0 + s_1 x + s_2 x^2 + \cdots \nonumber \\
    &= \sum_{k=0}^{\infty} s_{k} x^{k}.
    \label{eqn:vector layer activation}
\end{align}

Suppose that we can represent each entry of $\mathbf{x}$ as a power series of the entries of a vector $\mathbf{z} \in \mathbb{R}^{w}, (w \in \mathbb{Z})$.

\begin{align}
    \forall i &= 1, \cdots, d, \nonumber \\ x_i
    &= a^{(i)}_{0,0,\cdots,0} + a^{(i)}_{1,0,\cdots,0} z_1 + a^{(i)}_{2,0,\cdots,0} z_1^2 + \cdots \nonumber \\
    &+ a^{(i)}_{0,1,\cdots,0} z_2 + a^{(i)}_{1,1,\cdots,0} z_1 z_2 + a^{(i)}_{2,1,\cdots,0} z_1^2 z_2 + \cdots \nonumber \\
    &+ a^{(i)}_{0,0,\cdots,1} z_w + a^{(i)}_{1,0,\cdots,1} z_1 z_w + a^{(i)}_{2,0,\cdots,1} z_1^2 z_w + \cdots \nonumber \\
    &= \sum_{n_1,n_2,\cdots,n_w}^{\infty,\infty,\cdots,\infty} a^{(i)}_{n_1,n_2,\cdots,n_w} z_1^{n_1} z_2^{n_2} \cdots z_w^{n_w}.
    \label{eqn:vector x series}
\end{align}

Finally, suppose that we have the relation

\begin{equation}
    \mathbf{y} = \mathbf{\sigma}(\mathbf{\Theta} \mathbf{x})
    \label{eqn:vector layer relation}
\end{equation}

where $\sigma$ is being applied element-wise to a vector.\\

Similar to the scalar case, this represents a multi-neuron layer of an ANN with input $\mathbf{x}$, weights $\mathbf{\theta}$, output $\mathbf{y}$, and activation $\sigma$. The value of $z$ can be intrepreted as the input to the entire ANN.

\subsubsection{Objective}
We wish to represent each entry of $\mathbf{y}$ as a power series of the entries of $\mathbf{z}$. That is,

\begin{align}
    \forall i &= 1, \cdots, c, \nonumber \\ y_i
    &= b^{(i)}_{0,0,\cdots,0} + b^{(i)}_{1,0,\cdots,0} z_1 + b^{(i)}_{2,0,\cdots,0} z_1^2 + \cdots \nonumber \\
    &+ b^{(i)}_{0,1,\cdots,0} z_2 + b^{(i)}_{1,1,\cdots,0} z_1 z_2 + b^{(i)}_{2,1,\cdots,0} z_1^2 z_2 + \cdots \nonumber \\
    &+ b^{(i)}_{0,0,\cdots,1} z_w + b^{(i)}_{1,0,\cdots,1} z_1 z_w + b^{(i)}_{2,0,\cdots,1} z_1^2 z_w + \cdots \nonumber \\
    &= \sum_{n_1,n_2,\cdots,n_w}^{\infty,\infty,\cdots,\infty} b^{(i)}_{n_1,n_2,\cdots,n_w} z_1^{n_1} z_2^{n_2} \cdots z_w^{n_w}.
    \label{eqn:vector y series}
\end{align}

Rewriting Equation (\ref{eqn:vector layer relation}) in terms of Equations (\ref{eqn:vector layer activation}) and (\ref{eqn:vector x series}), we obtain

\begin{align}
    \forall i &= 1, \cdots, c, \nonumber \\ y_i
    &= \left[\mathbf{\sigma}(\mathbf{\Theta} \mathbf{x})\right]_i \nonumber \\
    &= \sigma(\mathbf{\theta}_i \mathbf{x}) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k (\mathbf{\theta}_i \mathbf{x})^k \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{j=1}^{d} \theta_{ij} x_{j}\right)^k \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} (\theta_{ij} x_j)^{k_j} \right) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d}\theta_{ij}^{k_j} \left(\sum_{n_1, \cdots, n_w}^{\infty,
    \cdots, \infty} a^{(j)}_{n_1,\cdots,n_w} z_1^{n_1}  \cdots z_w^{n_w} \right)^{k_j}\right) \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \left(\sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} \theta_{ij}^{k_j} \left(\sum_{\lVert \mathbf{L} \rVert_1 = k_j} \binom{k_j}{\mathbf{L}} \prod_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} (a^{(j)}_{n_1, \cdots, n_w} z_1^{n_1} \cdots z_w^{n_w})^{l_{n_1, \cdots, n_w}} \right)\right). \nonumber \\
    \label{eqn:vector y expansion}
\end{align}

where $\mathbf{L}$ is a collection of non-negative integers that are indices such that

\begin{align*}
    &\mathbf{L} = l_{\underbrace{0, \cdots, 0}_{\times w}}, \ldots, l_{\underbrace{\infty, \cdots, \infty}_{\times w}}, \\
    &\lVert \mathbf{L} \rVert_1 = l_{0, \cdots, 0} + \cdots + l_{\infty, \cdots, \infty}, \\
    &\binom{k_j}{\mathbf{L}} = \binom{k_j}{l_{0, \cdots, 0}, \ldots, l_{\infty, \cdots, \infty}}.
\end{align*}

\subsubsection{Coefficient Extraction}

To find the coefficients $b^{(i)}_{m_1, \cdots, m_w}$ from Equation (\ref{eqn:vector y expansion}), we must find terms satisfying index constraints

\begin{align}
    k &= \sum_{n=1}^{d} k_n \\
    k_j &= \lVert \mathbf{L} \rVert_1
\end{align}

and power constraints

\begin{align}
    m_1 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_1 l_{n_1, \cdots, n_w} \nonumber \\
    m_2 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_2 l_{n_1, \cdots, n_w} \nonumber \\
    \vdots \;\; &= \qquad \vdots \nonumber \\
    m_3 &= \sum_{n_1, \cdots, n_w}^{\infty, \cdots, \infty} n_w l_{n_1, \cdots, n_w}.
\end{align}

Similar to the scalar case, notice that the power constraints can be simplified since any solution must also satisfy

\begin{equation}
    l_{n_1, \cdots, n_{p-1}, m_p + 1, n_{p+1}, \cdots, n_w} = l_{n_1, \cdots, n_{p-1}, m_p + 2, n_{p+1}, \cdots, n_w} = \cdots = 0.
\end{equation}

resulting in

\begin{align}
    m_1 &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_1 l_{n_1, \cdots, n_w} \nonumber \\
    m_2 &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_2 l_{n_1, \cdots, n_w} \nonumber \\
    \vdots \;\; &= \qquad \vdots \nonumber \\
    m_w &= \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_w l_{n_1, \cdots, n_w}.
\end{align}

If a collection of $k_1, \ldots, k_d$ and $\mathbf{L}$ satisfy all of these constraints, then the value

\begin{equation*}
    s_k \binom{k}{k_1, \cdots, k_d} \binom{k_j}{\mathbf{L}} \theta_{ij}^{k_j} (a^{(j)}_{n_1, \cdots, n_w})^{l_{n_1, \cdots, n_w}}
\end{equation*}

may be pulled out to the sum of $b^{(i)}_{m_1, \cdots, m_w}$. In concise terms we have derived

\begin{equation}
    b^{(i)}_{m_1, \cdots, m_w} = \sum_{k=0}^{\infty} s_k \sum_{k_1 + \cdots + k_d = k} \binom{k}{k_1, \cdots, k_d} \prod_{j=1}^{d} \theta_{ij}^{k_j} \sum_{\substack{l_{0, \cdots, 0} + \cdots + l_{\infty, \cdots, \infty} = k_j \\ \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_1 l_{n_1, \cdots, n_w} = m_1 \\ \vdots \\ \sum_{n_1, \cdots, n_w}^{m_1, \cdots, m_w} n_w l_{n_1, \cdots, n_w} = m_w}} \binom{k_j}{\mathbf{L}} (a^{(j)}_{n_1, \cdots, n_w})^{l_{n_1, \cdots, n_w}}
\end{equation}

The only issue is that of finding solutions to the index and power constraints.
%End of vector coefficients derivation. %

%End of content moved to appendix from main body by Andrew and Maria on April 29th. %


%Beginning of previous appendix content.%
\subsection{Activation Functions}
\subsubsection{The logistic sigmoid}
\begin{equation}
	\sigma(x) = \frac{1}{1 + e^{-x}}
	\label{eqn:sigmoid}
\end{equation}
is a special case of the generating function for the Euler polynomial coefficients,
\begin{equation}
	\frac{2e^{x t}}{1 + e^t} = \sum_{n=0}^\infty E_n(x) \frac{t^n}{n!}
\end{equation}
where, for $x = 0$,
\begin{equation}
	\sigma(x) = \frac{1}{2} \sum_{n=0}^\infty E_n(0) \frac{(-1)^n}{n!}.
	\label{eqn:sigmoid Euler expansion}
\end{equation}

The Euler polynomials at $x=0$,
\begin{equation}
	E_n(0) = -2(n+1)^{-1} \left( 2^{n+1} - 1 \right) B_{n+1}
\end{equation}
where $B_n$ is the $n^\textrm{th}$ Bernoulli number. Since Bernoulli numbers of odd index, with the exception of $B_1$, are zero, $E_i(0) = 0$ for $i = 2, 4, 6, \ldots, 2n$. Therefore, the summand and limits of Equation~(\ref{eqn:sigmoid Euler expansion}) change to
\begin{equation}
	\sigma(x) = \frac{1}{2} - \frac{1}{2} \sum_{n=1}^\infty \left( \frac{E_{2n-1(0)}}{(2n-1)!} \right) x^{2n-1}.
\end{equation}

The series representation of $E_{2n-1}(x)$
\begin{equation}
	E_{2n-1}(x) = \frac{(-1)^n 4 (2n - 1)!}{\pi^{2n+1}} \sum_{k=0}^\infty \frac{\cos [(2k + 1) \pi x]}{(2k + 1)^{2n}}
\end{equation}
such that,
\begin{equation}
	E_{2n-1}(0) = \frac{(-1)^n 4 (2n - 1)!}{\pi^{2n+1}} \sum_{k=0}^\infty \frac{1}{(2k + 1)^{2n}}
\end{equation}
and therefore,
\begin{eqnarray}
	\sigma(x) & = & \frac{1}{2} - \sum_{n=1}^\infty 2 \frac{(-1)^n}{\pi^{2n}} \left( \sum_{k=0}^\infty \frac{1}{(2k+1)^{2n}} \right) x^{2n-1} \\
		& = & \frac{1}{2} - \sum_{n=1}^\infty 2 \frac{(-1)^n}{\pi^{2n}} \left( 4^{-n} \left( 4^n - 1 \right) \zeta(2n) \right) x^{2n-1} \nonumber \\
		& = & \frac{1}{2} - \sum_{n=1}^\infty \underbrace{2 \left( \frac{-1}{4\pi^2} \right)^n \left( 4^n - 1 \right) \zeta(2n)}_{a_n} x^{2n-1} \nonumber \\
		& = & \sum_{n=0}^\infty a_n x^n,\ a_n = \left\{ \begin{array}{l l}
			1/2	& n = 0 \\
			-2 \left( \frac{-1}{4\pi^2} \right)^{(n+1)/2} \left( 4^{(n+1)/2} - 1 \right)\zeta(n+1)	& n\ \text{odd} \\
			0	& n\ \text{even}
		\end{array}\right.
		\label{eqn:sigmoid zeta expansion}
\end{eqnarray}

\subsection{Coefficient Generating Functions for Common Functions}

Central to this approach is the connection ability to represent a constitutive relationship and a neural network on the same vector/covector space. This is done through a polynomial series expansion of both the neural network, covered in the text, and the constitutive relationship. Select generating functions are provided here.

\begin{sidewaystable}[htp]
\caption{Examples of coefficient generating functions for functional forms commonly found in materials physics.}
\begin{center}
\begin{tabular}{c | c c c c c c}
	k	& %
		$C a^x$	& %
			$C x^n$	& %
				$Ce^{-\beta x}$	& %
					$C x^{-1/2}$ & %
                        $C (1 + x)^\alpha$ & %
                            $C \ln(1 + x)$ \\[2ex]
	\hline
	0	& %
		$1$	& %
			--	& %
				$C$	& %
					$C$ & %
                        $C$ & %
                            $0$ \\[2ex]
	1	& %
		$C \ln a$	& %
			--	& %
				$-\beta C$		& %
					$-\frac{1}{2}C$ & %
                        $C\alpha$ & %
                            $C$ \\[2ex]
	2	& %
		$\frac{(\ln a)^2}{2} C$	& %
			--	&  %
				$\frac{\beta^2}{2} C$	& %
					$\frac{3}{8}C$	& %
                        $C\frac{\alpha (\alpha - 1)}{2!}$ & %
                            $\frac{-C}{2}$ \\[2ex]
	\vdots & \multicolumn{4}{c}{\vdots} \\[2ex]
	n	& %
		$\frac{(\ln a)^n}{n!} C$	& %
			$\left\{\begin{array}{c l}
				C & \text{if}\ k = n \\
				0 & \text{otherwise}
			  \end{array}\right.$	& %
				$(-1)^n\frac{\beta^n}{n!} C$	& %
					$C \prod_{i=1}^n (-1)\frac{2i - 1}{2i}$ & %
                        $C \frac{\prod_{i=1}^n \alpha - n + 1}{n!}$ & %
                            $C\frac{(-1)^{n+1}}{n}$ \\[2ex]
	\hline
    Constraint & %
        & %
            & %
                & %
                    & %
                        $-1 < x < 1$ & %
                            $-1 < x \le 1$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:generating functions of common functions}
\end{sidewaystable}

%Beginning of scalar coefficients worked examples.%
\subsection{Coefficients of the Scalar Polynomial Series}
\label{sec:worked scalar coeffs}

A result of the coefficient derivation of polynomial series is that we may construct formulae for the coefficients of an arbitrary function by simply solving for the constraints in Equation (\ref{eqn:scalar power series result}). We provide a few instances of worked coefficients here. 

We find $b_0$. $i = 0 \implies k_1 = k_2 = \cdots = 0$ so $k_0 = k$. Therefore,

\begin{align}
    b_0 
    &= \sum_{k=0}^{\infty} s_k \theta^k \sum_{\substack{k_0 + k_1 + \cdots = k \\ 0 k_0 + 1 k_1 + \cdots = 0}} \binom{k}{k_0, k_1, \cdots} \prod_{n=0}^{\infty} a_n^{k_n} \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \theta^k \binom{k}{k} a_0^{k} \nonumber \\
    &= \sum_{k=0}^{\infty} s_k \theta^k a_0^k
\end{align}

We find $b_1$. $i = 1 \implies k_2 = k_3 = \cdots = 0$ so $k_0 = k - 1$ and $k_1 = 1$. Further, $k \geq 1$. Therefore,

\begin{align}
    b_1
    &= \sum_{k=0}^{\infty} s_k \theta^k \sum_{\substack{k_0 + k_1 + \cdots = k \\ 0 k_0 + 1 k_1 + \cdots = 1}} \binom{k}{k_0, k_1, \cdots} \prod_{n=0}^{\infty} a_n^{k_n} \nonumber \\
    &= \sum_{k=1}^{\infty} s_k \theta^k \binom{k}{k - 1, 1} a_0^{k - 1} a_1 \nonumber \\
    &= \sum_{k=1}^{\infty} k s_k \theta^k a_0^{k-1} a_1
\end{align}

We find $b_2$. For this coefficient, there are multiple solutions to the constraints. 

\begin{itemize}
    \item $i = 2 \implies k_3 = k_4 = k_5 = \cdots = 0$ and either
    \begin{itemize}
        \item $k_2 = 0, k_1 = 2, k_0 = k - 2$ and $k \geq 2$ or
        \item $k_2 = 1, k_1 = 0, k_0 = k - 1$ and $k \geq 1$
    \end{itemize}
\end{itemize}

Therefore,

\begin{align}
    b_2
    &= \sum_{k=0}^{\infty} s_k \theta^k \sum_{\substack{k_0 + k_1 + \cdots = k \\ 0 k_0 + 1 k_1 + \cdots = 2}} \binom{k}{k_0, k_1, \cdots} \prod_{n=0}^{\infty} a_n^{k_n} \nonumber \\
    &= \sum_{k=2}^{\infty} s_k \theta^k \left(\binom{k}{k - 2, 2, 0}a_0^{k-2} a_1^{2}\right) + \sum_{k=1}^{\infty} s_k \theta^k \left(\binom{k}{k - 1, 0, 1}a_0^{k-1} a_2\right) \nonumber \\
    &= s_1 \theta a_2 + \sum_{k=2}^{\infty} s_k \theta^k \left(\frac{k(k-1)}{2} a_0^{k-2}a_1^{2} + k a_0^{k-1}a_2\right)
\end{align}
%End of scalar coefficients worked examples.%
%End of previous appendix content. %