\section{Introduction}\label{introduction}

With apologies to the Bard of Avon, \\

\vspace{2mm}
\noindent
\textit{Two methods, both alike in dignity, \\
In fair sciences, where we lay our scene. \\
From ancient elegance break to methods mutinous, \\
Where machine learning makes physical laws unclean, \\
From forth the noble loins of these two foes\\
A pair of cross-term'd numbers seek their place;\\
Whose serial expansions, equally covariant\\
Do with their projections, bury their parents strife.\\
Our fearful assessment of their dual-space truth,\\
And the continuance of their parents, sage,\\
Which, for their children's union, sought to achieve,\\
Is now the two hours' traffic of our stage;\\
The which, if you with patent eyes attend,\\
What here shall miss, our toil shall strive to mend.\\}

\vspace{2mm}

With the proliferation of increasingly powerful machine learning strategies, and their promise of the automatic discovery of new physics, phenomena, properties, and correlations, it is seductive to trust machine learning to solve our most complex problems. The unintended consequence of relegating one's data to machine learning is that our native physical understanding of currently-understood phenomenon may atrophy, and our development of descriptions of new physics may stagnate.  These risks could have profound impacts on numerous scientific fields. Consider the instruction of machine learning, where students, and hence future scientists, engineers and educators, may, unintentionally, understand machine learning to be a cudgel against which every problem which has a solution falls. When one tool is so powerful as to render other methods seemingly obsolete, it is natural to ``disarm'' oneself of the tools that seem obsolete.  Yet, such tools are not obsolete.  Various physical phenomenon are exceptionally well described by laws (the laws of Thermodynamics, kinetics, motion, ...) or theories (e.g., dislocations which underpin our understanding of strength in metals and alloys).  Those laws and theories permit us to creatively explore spaces where data does not exist, hypothesize, theorize, and test our hypotheses.  Should mastery of our understanding of physical processes suffer, the scientific method in its currently and widely accepted form, becomes harder to apply.  Thus, we must: retain mastery of current theory; postulate logically new theories; and leverage mathematical powers to understand complex hyperspaces describing n-variables. \\

The problem of whether to apply our most powerful tools in our analytical arsenal to a problem, or rigorously develop theories or calculate previously unknown physical constants seems to become an either-or problem.  Much like the houses of the Montagues and Capulets in Shakespeare, the tools of Machine Learning and Phenomenological Relationships seem irrenconcilable. Yet, the problem lies with neither approach (both alike in dignity), rather it stems from two very human limitations. Firstly, we have great difficulty interpreting problems in higher-order space.  It is easy to understand problems which have a single independent variable against which a phenomena depends, resulting in y-x plots of data.  It is not much harder to understand problems which have two independent variables against which a phenomena depends, as these can be represented in Euclidean space, and with imagination we can understand problems with three independent variables (say x,y and time) against which a phenomena (say, z=f(xy,t)) depends.  Beyond this, we generally lack tools to understand or visualize an n-dimensional hyperspace.  Secondly, we can develop an implicit bias when wrestling with mathematical expressions, and may begin to believe that one (and hence, only one) functional expression can be used to fit a relationship.  We develop mathematical ``favorites'', tools that, quite rationally, we turn to when solving certain problems.  We turn to these expressions for two reasons: they work, and they are reduced to the simplest form possible.  In materials science, against which the framework presented in this paper has been conceived it is common to turn to the Arrhenius relationship for phenomena which have temperature dependencies.  This simple form, $A = A_O\exp{-\frac{Q}{kT}}$, is taught extensively in a wide variety of classes, and yet, there is nothing in nature that dictates the exponential form be used.  It is used because it is the simplest known mathematical expression to fit the observed data.  And yet, it could be expanded in, e.g., polynomial form\ldots
\begin{align}
    A &= \sum_{m=0}^\infty a_m x^m \nonumber\\
        &= \sum_{m=0}^\infty \frac{(-1)^m}{m!} \left(\frac{Q}{kT}\right)^m
\label{eqn:coefficient generating function example}
\end{align}
\noindent where $a_m = (-1)^m/m!$ is the coefficient generating function and $x = Q/kT$ is the non-dimensionalized temperature.

If this recognition that our most trusted expressions can be re-written in another form, then so too must the reduced functions describing the most flexible and powerful neural networks.  The next logical step is to postulate that these expanded forms of different representations of the same physical phenomena can be compared, and assessed for their self-similarity, providing a \textit{translation} between one form and another.  A Rosetta Stone, if you will, where the mathematical functions (i.e., the language) in one reference frame can be translated and assessed for equivalency to the mathematical functions of the second reference frame.

The precursors to this thought process began when H.L. Fraser et al. began conducting so-called virtual experiments, which permitted a well-trained neural network to be probed to determine the influence of one variable on a physical property while all of the other potential independent variables were artificially held at their average values [refs]. These virtual experiments were slices through an n-dimensional hyperspace, and it became obvious that the lower dimension slices could be described using simpler functions than the full neural network.  This early work was built upon the initial efforts by H.K.D.H. Bhadeshia to solve complex materials science problems using artificial neural networks [refs] and the work of D.J.C. MacKay [refs] to incorporate Bayesian statistics and feedback loops.  Following this prior work, with the assumption that a mapping was possible between the basis vectors of the n-variable hyperspace equally represented by the expansion of terms of an artificial neural network and the basis vectors representing the physical processes, Ghamarian and Collins, seeking to establish a constitutive equation for the room temperature yield strength of a titanium alloy given variations in its compositional and microstructural states, applied a hybrid artificial neural network-genetic algorithm method that optimized the unknowns in a postulated physically-based equation, testing the optimized physically-based model against slices through hyperspace function representing the neural network model and the data [refs]. This latter effort was quite inefficient, but resulted in a physically-based equation that was demonstrated, in subsequent work, to be generalized for multiple different variations of processing and with compositional variations [refs].

This previous work strongly suggests that the hypothesis we propose below is valid, and that a more fulsome mathematical treatment is merited and which is the subject of this paper.  We recognize that this approach lies neither fully in a mathematics space nor in an engineering/science space. Consequently, we aim to provide both sufficiency of the derivations and sufficiency of motivation and impact, so that the paper is accessible to readers of various communities.  While we develop the approach based upon a materials science problem, we hope that the general applicability will be apparent, as it is easy to imagine this approach impacting a diverse range of disciplines, including: genetics, public health, biological sciences, earth sciences, information sciences (including signals analysis), physical sciences, applied variants thereof (medicine, environmental activities, engineering), space sciences, and economics.  We finally include an appendix that contains the expansions of the activation functions and the generating functions of common functions that form the basis of some relevant physics.  We recognize that this appendix is far from complete, but hope that the logic presented will permit those interested in identifying and developing additional functions as the specific applications demand.

\begin{hypothesis}
We hypothesize that the physics of an arbitrarily complex process can be extracted by fitting the series expansion of known, postulated, or potential physical relationship to the coefficients of a series expansion of a trained neural network when those series are supported on the same basis.
\end{hypothesis}

Coefficient generating functions, such as that shown in Equation~\ref{eqn:coefficient generating function example}, have been tabulated for a number of common mathematical expressions, c.f. Table~\ref{tab:generating functions of common functions}. For constitutive relationships composed of a superposition of these terms, the series expansion coefficients become an analytical expression in terms of the physical constants of the constitutive relationships: the Hall-Petch coefficient, $k$; the Young's modulus, $E$; etc. In general, we represent these coefficients as $C_k$ throughtout this work. Any physical constants known a priori reduce the number of $C_k$ in the final coefficient generating function expression.
