\section{Discussion}\label{discussion}

% Find \alpha_k for neural network (NN) expansion
% Find \beta_k for constitutive relationship (CR) expansion
% Solve for C_k through fit (least-squares for now) of \alpha_k to \beta_k

% Hadamard (element-wise) product does not distribute.
In the solution of the

% data pre-processing
To avoid implicit bias, data preprocessing is an important first step in training a neural network. Commonly, data is whitened, also known as scaling or standardizing, ${\bf x}_s: {\bf x}_s = \frac{{\bf x} - \overline{\bf x}}{\sigma}$, where $\overline{\bf x}$ is the arithmetic mean and $\sigma$ is the standard deviation of the data set, ${\bf x}$. However, a model trained on such scaled data would change the space such that the primal space of the model (scaled) would differ from the primal space of the constitutive relationship (unscaled).

To accommodate whitened data, which is necessary to remove implicit bias while training a neural network, the constitutive relationship must also operate in the scaled primal space. After model training and fit of the physical constants in the constitutive relationship from $\boldsymbol{\alpha}$, the dual space of the neural network expansion, and $\boldsymbol{\beta}$, the dual space of the constitutive relationship expansion, the inverse transformation should be applied to closed-form constitutive relationship to extract the physical constant, e.g.
\begin{align*}
    f(x) &= C_0 + C_1 x_s \\
        &= C_0 + C_1 \left( \frac{x - \overline{x}}{\sigma} \right) \\
        &= C_0^\prime + C_1^\prime x
\end{align*}
where
\begin{conditions}
    C_0^\prime & $C_0 - C_1 \overline{x}/\sigma$ \\
    C_1^\prime & $C_1/\sigma$
\end{conditions}
Note that this accommodation may introduce a bias/offset even in constitutive relationships where no such offset was present before.

% comment on the distance metric
The Euclidean ($L_2\textrm{-norm}$) distance is used in optimizations, such as the training of neural networks and other machine learning algorithms, through the mean-square error,
\begin{equation*}
    \argmin_{\boldsymbol{\theta}} \|{\bf y} - \boldsymbol{\theta}{\bf x}.
\end{equation*}
However, because of the curse of dimensionality, the $L_2\textrm{-norm}$ is not a desirable metric in high-dimensional spaces.

% original
Many non-trivial problems in materials science, and in science more broadly, are explained not through a single constitutive relationship, but through a superposition of contributing physics.

Figure~\ref{fig:stress} shows an artificial dataset constructed to replicate the impact of yield stress in a two-phase, solid-solution strengthened alloy system. Using a combination of composite theory for the contribution of flow stress, {\color{red} NAME} solid solution \cite{solid solution}, and Hall-Petch \cite{Hall-Petch} strengthening, the expected yield stress is

\begin{equation}
	\sigma_y = F_v^A \sigma_f^A + F_v^B \sigma_f^B + \sum_i C_i [x_i]^{2/3} + \sum_j k_j d_j^{-1/2} + \ldots
\end{equation}

\noindent with free parameters \\[2ex]
\begin{tabular}{l l}
	$F_v^i$	& Volume fraction of phase $i$ \\
	$[x_i]$	& Concentration of solute $i$ \\
	$d_j$	& Average grain diameter of phase $j$
\end{tabular}
\\[2ex]
\noindent and fixed parameters \\[2ex]
\begin{tabular}{l l}
	$\sigma_f^i$	& Flow stress of phase $i$ \\
	$C_i$		& Solid solution strengthening coefficient for solute species $i$ \\
	$k_j$		& Hall-Petch strengthening coefficient for phase $j$
\end{tabular}
\\[2ex]

The goal is to iteratively improve on this constitutive model one term at a time, and monitor the effect on the residuals between the predicted yield, $\hat{\sigma_y}$ and the actual yield $\sigma_y$.

Together, this leads to a seven-step process for systematically and incrementally extracting physics information from an ANN:
\begin{enumerate}
	\item Collect data--features and targets--for which relationships are expected to exist.
	\item Design and train a fully dense multi-layer perceptron network (ANN).
	\item Build a power series expansion from the architecture of this ANN, using Equations~(\ref{eqn:sigmoid zeta expansion}) and (\ref{eqn:ANN power series coefficient generating function}) to populate the coefficients using the trained weights from the neural network.
	\item Hypothesize a constitutive relationship between the feature space and the target space. \label{item:hypothesis}
	\item Recast the terms in the hypothesis function from \#\ref{item:hypothesis} as power series expansions, creating power series coefficient generating functions that are functions of the constitutive model fitting parameters. An example of this process is provided below, and a table of select power series expansions relevant to materials research are provided in Table~(\ref{table:power series expansions}). \label{item:coefficients}
	\item Perform an optimization, \emph{e.g.} least squares, fit to find the fitting parameters from \#\ref{item:coefficients}
	\item Calculate the residuals of the ANN power series expansion coefficient vector, and from this residual vector, the error in the model. If the accuracy is sufficient for the application, stop; otherwise, expand the constitutive relationship from step \#\ref{item:hypothesis} and repeat.
\end{enumerate}

\begin{table}[htp]
\caption{Examples of coefficient generating functions for functional forms commonly found in materials physics.}
\begin{center}
\begin{tabular}{c | c c c c}
	k	& %
		$C a^x$	& %
			$C x^n$	& %
				$Ce^{-\beta x}$	& %
					$C x^{-1/2}$ \\[2ex]
	\hline
	0	& %
		$1$	& %
			--	& %
				$C$	& %
					$C$ \\[2ex]
	1	& %
		$C \ln a$	& %
			--	& %
				$-\beta C$		& %
					$-\frac{1}{2}C$ \\[2ex]
	2	& %
		$\frac{(\ln a)^2}{2} C$	& %
			--	&  %
				$\frac{\beta^2}{2} C$	& %
					$\frac{3}{8}C$	\\[2ex]
	\vdots & \multicolumn{4}{c}{\vdots} \\[2ex]
	n	& %
		$\frac{(\ln a)^n}{n!} C$	& %
			$\left\{\begin{array}{c l}
				C & \text{if}\ k = n \\
				0 & \text{otherwise}
			  \end{array}\right.$	& %
				$(-1)^n\frac{\beta^n}{n!} C$	& %
					$C \prod_{i=1}^n (-1)\frac{2i - 1}{2i}$ \\[2ex]
	\hline
\end{tabular}
\end{center}
\label{tab:generating functions of common functions}
\end{table}%


%\noindent {\color{red} How does dropout affect this? It doesn't. Dropout simply sets specific $\boldsymbol\theta$ to zero, which is handled seamlessly in the previous treatment.}
