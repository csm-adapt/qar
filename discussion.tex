\section{Discussion}\label{discussion}

Many non-trivial problems in materials science, and in science more broadly, are explained not through a single constitutive relationship, but through a superposition of contributing physics. 

Figure~\ref{fig:stress} shows an artificial dataset constructed to replicate the impact of yield stress in a two-phase, solid-solution strengthened alloy system. Using a combination of composite theory for the contribution of flow stress, {\color{red} NAME} solid solution \cite{solid solution}, and Hall-Petch \cite{Hall-Petch} strengthening, the expected yield stress is

\begin{equation}
	\sigma_y = F_v^A \sigma_f^A + F_v^B \sigma_f^B + \sum_i C_i [x_i]^{2/3} + \sum_j k_j d_j^{-1/2} + \ldots
\end{equation}

\noindent with free parameters \\[2ex]
\begin{tabular}{l l}
	$F_v^i$	& Volume fraction of phase $i$ \\
	$[x_i]$	& Concentration of solute $i$ \\
	$d_j$	& Average grain diameter of phase $j$
\end{tabular}
\\[2ex]
\noindent and fixed parameters \\[2ex]
\begin{tabular}{l l}
	$\sigma_f^i$	& Flow stress of phase $i$ \\
	$C_i$		& Solid solution strengthening coefficient for solute species $i$ \\
	$k_j$		& Hall-Petch strengthening coefficient for phase $j$
\end{tabular}
\\[2ex]

The goal is to iteratively improve on this constitutive model one term at a time, and monitor the effect on the residuals between the predicted yield, $\hat{\sigma_y}$ and the actual yield $\sigma_y$.

The analytical form, combining Equations~(\ref{eqn:nn analytical form}) and (\ref{eqn:sigmoid zeta expansion}), the estimated output of a two-layer NN can be written as

\begin{eqnarray}
	{\bold y}_1 & = & \sum_{k=0}^\infty a_k (\boldsymbol\theta_1^T {\bold x})^k \nonumber \\
	{\bold y}_2 & = & \sum_{k=0}^\infty b_k (\boldsymbol\theta_2^T {\bold y}_1)^k \nonumber \\
		& = & b_0 {\bold 1} + \nonumber\\
		&   & + b_1 (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_2 + %
							(\tilde a_3 + %
								(\ldots) \tilde {\bold x} )%
							\tilde {\bold x} )%
						\tilde {\bold x} )%
					\tilde {\bold x}) \nonumber \\
		&   & + b_2 (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_2 + %
							(\tilde a_3 + %
								(\ldots) \tilde {\bold x} )%
							\tilde {\bold x} )%
						\tilde {\bold x} )%
					\tilde {\bold x})^2 \nonumber \\
		&   & \vdots \nonumber \\
		&   & + b_k (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_2 + %
							(\tilde a_3 + %
								(\ldots) \tilde {\bold x} )%
							\tilde {\bold x} )%
						\tilde {\bold x} )%
					\tilde {\bold x})^k \nonumber \\
		&   & \vdots
\end{eqnarray}
\noindent where $\tilde a_i = \boldsymbol\theta_2^T a_i$ and $\tilde{\bold x} = \boldsymbol\theta_1^T {\bold x}$. All $\boldsymbol\theta_i$, ${\bold x}$, and ${\bold y}$ are augmented to include the bias, ${\bold b}_i$, that is,
\begin{eqnarray}
	{\bold x}&:& {\bold x} \leftarrow \begin{pmatrix}
								1 \\
								{\bold x}
							\end{pmatrix} = \begin{pmatrix}
											1 \\
											x_0 \\
											x_1 \\
											\vdots \\
											x_n
										\end{pmatrix}\\
	{\bold y}&:& {\bold y} \leftarrow \begin{pmatrix}
								1 \\
								{\bold y}
							\end{pmatrix} \\
	\boldsymbol\theta_i&:& \boldsymbol\theta_i \leftarrow \begin{pmatrix} {\bold b}_i & \boldsymbol\theta_i \end{pmatrix}
\end{eqnarray}
However from Equation~(\ref{eqn:sigmoid zeta expansion}), $a_i  = 0\ \text{for}\ i = 2, 4, 6, \ldots$, and therefore,
\begin{eqnarray}
	{\bold y}_1 & = & \sum_{k=0}^\infty a_k (\boldsymbol\theta_1^T {\bold x})^k \nonumber \\
	{\bold y}_2 & = & \sum_{k=0}^\infty b_k (\boldsymbol\theta_2^T {\bold y}_1)^k \nonumber \\
		& = & b_0 {\bold 1} + \nonumber\\
		&   & + b_1 (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_3 + %
							(\tilde a_5 + %
								(\ldots) \tilde {\bold x}^2 )%
							\tilde {\bold x}^2 )%
						\tilde {\bold x}^2 )%
					\tilde {\bold x}) \nonumber \\
		&   & + b_2 (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_3 + %
							(\tilde a_5 + %
								(\ldots) \tilde {\bold x}^2 )%
							\tilde {\bold x}^2 )%
						\tilde {\bold x}^2 )%
					\tilde {\bold x})^2 \nonumber \\
		&   & \vdots \nonumber \\
		&   & + b_k (\tilde a_0 +%
					(\tilde a_1 + %
						(\tilde a_3 + %
							(\tilde a_5 + %
								(\ldots) \tilde {\bold x}^2 )%
							\tilde {\bold x}^2 )%
						\tilde {\bold x}^2 )%
					\tilde {\bold x})^k \nonumber \\
		&   & \vdots
\end{eqnarray}
\noindent where the $\tilde{\bold x}^2 = \tilde{\bold x} \odot \tilde{\bold x}$, the Hadamard product of $\tilde{\bold x}$ with itself.
\begin{eqnarray}
	{\bold y}_2 & = & \sum_{N=0}^\infty \sum_{k=0}^{N} \sum_{l=0}^{k} \sum_{m=0}^{l} \ldots %
		b_N %
		\binom{N}{k,l,m,\ldots} %
		\tilde a_0^k \tilde a_1^l \tilde a_3^m \ldots %
		\tilde {\bold x}^{N-k\ldots} %
		({\tilde {\bold x}^2})^{N-k-l\ldots} %
		({\tilde {\bold x}^2})^{N-k-l-m\ldots} \nonumber \\
	& = & \sum_{N=0}^\infty \sum_{k=0}^{N} \sum_{l=0}^{k} \sum_{m=0}^{l} \ldots %
		b_N %
		\binom{N}{k,l,m,\ldots} %
		\tilde a_0^k \tilde a_1^l \tilde a_3^m \ldots %
		\tilde {\bold x}^{l+m+n+\ldots} %
		({\tilde {\bold x}^2})^{m+n+\ldots} %
		({\tilde {\bold x}^2})^{n+\ldots}
	\label{eqn:ANN power series coefficient generating function}
\end{eqnarray}
\noindent where $k+l+m+n+\ldots = N$. Collecting coefficients and terms of power $k$,
\begin{equation*}
	{\bold y_2} =  \sum_{k=0}^\infty c_k \tilde{\bold x}^k
\end{equation*}
\noindent that, having the same form as Equation~(\ref{eqn:sigmoid zeta expansion}) creates a sequential process for determining the coefficients of the power series expansion of each layer in an ANN. Importantly, the output layer in a ANN regression is a single node with a linear activation, so the final layer, $y_f$, working from the last hidden layer, ${\bold y}_n$, is simply,
\begin{equation}
	y_f = \boldsymbol\theta_n^T {\bold y}_n
\end{equation}

Together, this leads to a seven-step process for systematically and incrementally extracting physics information from an ANN:
\begin{enumerate}
	\item Collect data--features and targets--for which relationships are expected to exist.
	\item Design and train a fully dense multi-layer perceptron network (ANN).
	\item Build a power series expansion from the architecture of this ANN, using Equations~(\ref{eqn:sigmoid zeta expansion}) and (\ref{eqn:ANN power series coefficient generating function}) to populate the coefficients using the trained weights from the neural network.
	\item Hypothesize a constitutive relationship between the feature space and the target space. \label{item:hypothesis}
	\item Recast the terms in the hypothesis function from \#\ref{item:hypothesis} as power series expansions, creating power series coefficient generating functions that are functions of the constitutive model fitting parameters. An example of this process is provided below, and a table of select power series expansions relevant to materials research are provided in Table~(\ref{table:power series expansions}). \label{item:coefficients}
	\item Perform an optimization, \emph{e.g.} least squares, fit to find the fitting parameters from \#\ref{item:coefficients}
	\item Calculate the residuals of the ANN power series expansion coefficient vector, and from this residual vector, the error in the model. If the accuracy is sufficient for the application, stop; otherwise, expand the constitutive relationship from step \#\ref{item:hypothesis} and repeat.
\end{enumerate}

\begin{table}[htp]
\caption{Examples of coefficient generating functions for functional forms commonly found in materials physics.}
\begin{center}
\begin{tabular}{c | c c c c}
	k	& %
		$C a^x$	& %
			$C x^n$	& %
				$Ce^{-\beta x}$	& %
					$C x^{-1/2}$ \\[2ex]
	\hline
	0	& %
		$1$	& %
			--	& %
				$C$	& %
					$C$ \\[2ex]
	1	& %
		$C \ln a$	& %
			--	& %
				$-\beta C$		& %
					$-\frac{1}{2}C$ \\[2ex]
	2	& %
		$\frac{(\ln a)^2}{2} C$	& %
			--	&  %
				$\frac{\beta^2}{2} C$	& %
					$\frac{3}{8}C$	\\[2ex]
	\vdots & \multicolumn{4}{c}{\vdots} \\[2ex]
	n	& %
		$\frac{(\ln a)^n}{n!} C$	& %
			$\left\{\begin{array}{c l}
				C & \text{if}\ k = n \\
				0 & \text{otherwise}
			  \end{array}\right.$	& %
				$(-1)^n\frac{\beta^n}{n!} C$	& %
					$C \prod_{i=1}^n (-1)\frac{2i - 1}{2i}$ \\[2ex]
	\hline
\end{tabular}
\end{center}
\label{tab:generating functions of common functions}
\end{table}%


%\noindent {\color{red} How does dropout affect this? It doesn't. Dropout simply sets specific $\boldsymbol\theta$ to zero, which is handled seamlessly in the previous treatment.}
